{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be6ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaden\\OneDrive\\Professional\\Athena\\Note Creation\\RAG\\virtual_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from create_kb import create_kb_from_file\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from dsrag.knowledge_base import KnowledgeBase\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c82dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    kb: KnowledgeBase\n",
    "    question: str\n",
    "    context: list[dict]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75d8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    search_queries = [state[\"question\"]]\n",
    "    retrieved_docs = state[\"kb\"].query(search_queries)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc[\"content\"] for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21248687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c024c40",
   "metadata": {},
   "source": [
    "### Stress test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa6e258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = [\n",
    "    \"cs168.txt\", \"econ136.txt\", \"mcb130.txt\", \"music139.txt\", \n",
    "    \"philosophy25b.txt\", \"physics7a.txt\", \"stat20.txt\"\n",
    "]\n",
    "transcript_to_queries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a query generation system. Please generate one or more search queries (up to a maximum of {max_queries}) that can be answered based on the provided section title and content. DO NOT generate the answer, just queries.\n",
    "\n",
    "Each of the queries you generate will be used to search a knowledge base for information that can be used to respond to the user input. Make sure each query is specific enough to return relevant information. If multiple pieces of information would be useful, you should generate multiple queries, one for each specific piece of information needed.\n",
    "\n",
    "Return a list of queries formatted as follows: [query1, query2]\n",
    "\n",
    "Example output: [\"Who is Napoleon?\", \"Where did Napoleon conquer?\"]\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        SYSTEM_MESSAGE\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Section title: {section_title}\n",
    "        Section content: {section_content}\n",
    "        \"\"\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "from dsrag.dsparse.sectioning_and_chunking.semantic_sectioning import get_sections_from_str\n",
    "import ast\n",
    "# generate important queries for a section\n",
    "def gen_queries(title: str, content: str, max_queries: int = 3) -> list[str]:\n",
    "    # a) Fill in the template\n",
    "    prompt_value = prompt_template.format_prompt(\n",
    "        section_title = title,\n",
    "        section_content = content,\n",
    "        max_queries = max_queries\n",
    "    )\n",
    "    # b) Turn that into LangChain messages\n",
    "    messages = prompt_value.to_messages()\n",
    "    # c) Ask the model\n",
    "    resp = llm(messages)            #\n",
    "    text = resp.content            \n",
    "    # d) Split into lines (or use your own parser if e.g. you return JSON)\n",
    "    return ast.literal_eval(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b643c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your raw text:\n",
    "for transcript in transcripts:\n",
    "    with open(transcript, encoding = \"utf-8-sig\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    sections, lines = get_sections_from_str(\n",
    "        document=raw,\n",
    "        max_characters_per_window=20000,   # how large each LLM window is\n",
    "        semantic_sectioning_config={\n",
    "            \"use_semantic_sectioning\": True,\n",
    "            \"llm_provider\": \"openai\",\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"language\": \"en\",\n",
    "        },\n",
    "        chunking_config={},\n",
    "        kb_id=\"my_kb\",\n",
    "        doc_id=\"my_doc\",\n",
    "    )\n",
    "\n",
    "    queries = []\n",
    "    for section in sections:\n",
    "        queries.extend(gen_queries(section[\"title\"], section[\"content\"]))\n",
    "    transcript_to_queries[transcript] = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed62bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a response for each query\n",
    "def gen_responses(transcript_index: int, sample: list[str]):\n",
    "    queries_and_responses = {}\n",
    "    for query in sample:\n",
    "        response = graph.invoke({\n",
    "            \"question\": query,\n",
    "            \"kb\": kbs[transcript_index]\n",
    "        })\n",
    "        queries_and_responses[query] = response[\"answer\"]\n",
    "    return queries_and_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample(transcript: str, n: int = 10):\n",
    "    return random.sample(transcript_to_queries[transcript], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create knowledge base from transcript\n",
    "kbs = []\n",
    "samples = {}\n",
    "for transcript in transcripts:\n",
    "    kbs.append(create_kb_from_file(transcript[:-4], transcript))\n",
    "    samples[transcript] = gen_sample(transcript)\n",
    "\n",
    "responses = []\n",
    "for i in range(len(samples)):\n",
    "    res = gen_responses(i, samples[transcripts[i]])\n",
    "    responses.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07e7392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for response in responses:\n",
    "    questions.extend(list(response.keys()))\n",
    "    answers.extend(list(response.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea68593",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Question\": questions,\n",
    "    \"LLM Answer\": answers\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2683afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(\"results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4654475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>LLM Answer</th>\n",
       "      <th>Human Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are optical fibers laid underwater?</td>\n",
       "      <td>Optical fibers are laid underwater using speci...</td>\n",
       "      <td>Installing optical fibers is the process of ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are loops in network routing?</td>\n",
       "      <td>Loops in network routing refer to situations w...</td>\n",
       "      <td>When a packet cycles around the same set of no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do participants determine their magic numb...</td>\n",
       "      <td>Participants determine their magic number by s...</td>\n",
       "      <td>Initially, participants’ magic number is infin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of routers in managing networ...</td>\n",
       "      <td>Routers manage network bandwidth by acting as ...</td>\n",
       "      <td>Routers enable a more efficient use of network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of a router?</td>\n",
       "      <td>The role of a router is to act as an intermedi...</td>\n",
       "      <td>A router is an intermediary node between diffe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0            How are optical fibers laid underwater?   \n",
       "1                 What are loops in network routing?   \n",
       "2  How do participants determine their magic numb...   \n",
       "3  What is the role of routers in managing networ...   \n",
       "4                      What is the role of a router?   \n",
       "\n",
       "                                          LLM Answer  \\\n",
       "0  Optical fibers are laid underwater using speci...   \n",
       "1  Loops in network routing refer to situations w...   \n",
       "2  Participants determine their magic number by s...   \n",
       "3  Routers manage network bandwidth by acting as ...   \n",
       "4  The role of a router is to act as an intermedi...   \n",
       "\n",
       "                                        Human Answer  \n",
       "0  Installing optical fibers is the process of ru...  \n",
       "1  When a packet cycles around the same set of no...  \n",
       "2  Initially, participants’ magic number is infin...  \n",
       "3  Routers enable a more efficient use of network...  \n",
       "4  A router is an intermediary node between diffe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff34d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70 entries, 0 to 69\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Question      70 non-null     object\n",
      " 1   LLM Answer    70 non-null     object\n",
      " 2   Human Answer  40 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.8+ KB\n"
     ]
    }
   ],
   "source": [
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5113e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd0032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaden\\OneDrive\\Professional\\Athena\\Note Creation\\RAG\\virtual_env\\lib\\site-packages\\networkx\\utils\\backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "c:\\Users\\Kaden\\OneDrive\\Professional\\Athena\\Note Creation\\RAG\\virtual_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kaden\\.cache\\huggingface\\hub\\models--sentence-transformers--stsb-roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2358d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_llm = model.encode(list(results[\"LLM Answer\"]), convert_to_tensor=True)\n",
    "embeddings_human = model.encode(list(results[\"Human Answer\"]))\n",
    "\n",
    "similarity = []\n",
    "\n",
    "for i in range(len(embeddings_llm)):\n",
    "    similarity.append(util.pytorch_cos_sim(embeddings_llm[i], embeddings_human[i]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee3c3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of response similarities: 0.6674860008060932\n",
      "standard deviation of response similarities: 0.14756425367320591\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"mean of response similarities: {np.mean(similarity)}\")\n",
    "print(f\"standard deviation of response similarities: {np.std(similarity)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e4ef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaden\\AppData\\Local\\Temp\\ipykernel_21960\\2204715469.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"response similarity\"] = similarity\n"
     ]
    }
   ],
   "source": [
    "results[\"response similarity\"] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44acc9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>LLM Answer</th>\n",
       "      <th>Human Answer</th>\n",
       "      <th>response similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are optical fibers laid underwater?</td>\n",
       "      <td>Optical fibers are laid underwater using speci...</td>\n",
       "      <td>Installing optical fibers is the process of ru...</td>\n",
       "      <td>0.625332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are loops in network routing?</td>\n",
       "      <td>Loops in network routing refer to situations w...</td>\n",
       "      <td>When a packet cycles around the same set of no...</td>\n",
       "      <td>0.531271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do participants determine their magic numb...</td>\n",
       "      <td>Participants determine their magic number by s...</td>\n",
       "      <td>Initially, participants’ magic number is infin...</td>\n",
       "      <td>0.785601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of routers in managing networ...</td>\n",
       "      <td>Routers manage network bandwidth by acting as ...</td>\n",
       "      <td>Routers enable a more efficient use of network...</td>\n",
       "      <td>0.855478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of a router?</td>\n",
       "      <td>The role of a router is to act as an intermedi...</td>\n",
       "      <td>A router is an intermediary node between diffe...</td>\n",
       "      <td>0.779942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0            How are optical fibers laid underwater?   \n",
       "1                 What are loops in network routing?   \n",
       "2  How do participants determine their magic numb...   \n",
       "3  What is the role of routers in managing networ...   \n",
       "4                      What is the role of a router?   \n",
       "\n",
       "                                          LLM Answer  \\\n",
       "0  Optical fibers are laid underwater using speci...   \n",
       "1  Loops in network routing refer to situations w...   \n",
       "2  Participants determine their magic number by s...   \n",
       "3  Routers manage network bandwidth by acting as ...   \n",
       "4  The role of a router is to act as an intermedi...   \n",
       "\n",
       "                                        Human Answer  response similarity  \n",
       "0  Installing optical fibers is the process of ru...             0.625332  \n",
       "1  When a packet cycles around the same set of no...             0.531271  \n",
       "2  Initially, participants’ magic number is infin...             0.785601  \n",
       "3  Routers enable a more efficient use of network...             0.855478  \n",
       "4  A router is an intermediary node between diffe...             0.779942  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6da25d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
